{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "import soundfile as sf\n",
    "import tqdm\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT =  \"../input/train_resampled\"\n",
    "OUTPUT = \"../output/train_npz\"\n",
    "SAMPLE_RATE = 32_000\n",
    "MU = 256\n",
    "NUM_WORKERS = cpu_count()\n",
    "\n",
    "print(NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spec(audio):\n",
    "    spec = librosa.power_to_db(\n",
    "        librosa.feature.melspectrogram(audio, sr=SAMPLE_RATE, fmin=20, fmax=16000, n_mels=128)\n",
    "    )\n",
    "    return spec.astype(np.float32)\n",
    "\n",
    "def audio2vec(path):\n",
    "    x, _ = sf.read(path)\n",
    "    x_spex = audio_to_spec(x)\n",
    "    np.save(f\"{OUTPUT}/{path.parent.name}/{path.name}.npz\", x_spex)\n",
    "    \n",
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "def audio2pict(path):\n",
    "    x, _ = sf.read(path)\n",
    "    x_spex = audio_to_spec(x)\n",
    "    cv2.imwrite(f\"{OUTPUT}/{path.parent.name}/{path.name}.jpg\", mono_to_color(x_spex))\n",
    "    \n",
    "def audio2quantized(path):\n",
    "    data, _ = librosa.load(path=path, sr=SAMPLE_RATE, mono=True)\n",
    "    mu_x = np.sign(data) * np.log(1 + MU * np.abs(data)) / np.log(MU + 1)\n",
    "    bins = np.linspace(-1, 1, MU)\n",
    "    quantized = np.digitize(mu_x, bins) - 1\n",
    "    quantized = quantized.astype(np.uint8)\n",
    "    np.save(f\"{OUTPUT}/{path.parent.name}/{path.name}.npz\", quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = defaultdict(list)\n",
    "for directory in tqdm.tqdm_notebook(Path(INPUT).iterdir(), total=len(os.listdir(INPUT))):\n",
    "    if directory.name == \".DS_Store\":\n",
    "        continue\n",
    "    !mkdir -p \"{OUTPUT}/{directory.name}\"\n",
    "    file_paths = [f for f in directory.iterdir() if f.name != \".DS_Store\"]\n",
    "    with Pool(NUM_WORKERS // 2) as p:\n",
    "        #p.map(audio2vec, file_paths)\n",
    "        #p.map(audio2pict, file_paths)\n",
    "        p.map(audio2quantized, file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check ignore files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in tqdm.tqdm_notebook(Path(OUTPUT).iterdir(), total=len(os.listdir(OUTPUT))):\n",
    "    if directory.name == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = [f for f in directory.iterdir() if f.name != \".DS_Store\"]\n",
    "    for path in file_paths:\n",
    "        size = os.path.getsize(path)\n",
    "        if size < 1:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    f\"{INPUT}/comrav/XC246425.wav\",\n",
    "    f\"{INPUT}/prawar/XC479026.wav\",\n",
    "    f\"{INPUT}/snobun/XC487557.wav\",\n",
    "    f\"{INPUT}/snobun/XC487556.wav\",\n",
    "    f\"{INPUT}/stejay/XC503349.wav\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = sf.read(paths[0])\n",
    "x_spex = audio_to_spec(x)\n",
    "\n",
    "print(x_spex.shape)\n",
    "cv2.imwrite(f\"tmp.jpg\", mono_to_color(x_spex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"../output/train_npz/aldfly/XC134874.wav.npz.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = librosa.feature.melspectrogram(arr.astype(float), sr=SAMPLE_RATE, fmin=20, fmax=16000, n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(spect.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(spect)\n",
    "x = x.unsqueeze(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.nn.MaxPool1d(32)(x)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h[0].numpy().max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h[0].numpy().max(0) > 0.1*1e7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = defaultdict(list)\n",
    "for directory in tqdm.tqdm_notebook(Path(INPUT).iterdir(), total=len(os.listdir(INPUT))):\n",
    "    if directory.name == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = [f for f in directory.iterdir() if f.name != \".DS_Store\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = file_paths[4]\n",
    "path = \"../input/train_resampled/ameavo/XC304534.wav\"\n",
    "print(path)\n",
    "data, sr = librosa.load(path=path, sr=SAMPLE_RATE, mono=True)\n",
    "plt.plot(data[:160000], ',', linestyle=\"None\");plt.show()\n",
    "\n",
    "x_spex = audio_to_spec(data)\n",
    "pct = mono_to_color(x_spex)\n",
    "plt.imshow(pct);plt.show()\n",
    "\n",
    "IPython.display.Audio(data=data, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "dt = 32000\n",
    "\n",
    "F = np.fft.fft(data)\n",
    "F_abs = np.abs(F)\n",
    "F_abs_amp = F_abs / N * 2\n",
    "\n",
    "fq = np.linspace(0, 1.0/dt, N)\n",
    "\n",
    "plt.xlabel('freqency(Hz)', fontsize=14)\n",
    "plt.ylabel('amplitude', fontsize=14)\n",
    "plt.plot(fq, F_abs_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 1e-5 # カットオフ（周波数）\n",
    "F[(fq > fc)] = 0\n",
    "\n",
    "#ac = 0.00002 # 振幅強度の閾値\n",
    "#F[(F_abs_amp < ac)] = 0\n",
    "\n",
    "F_abs = np.abs(F)\n",
    "F_abs_amp = F_abs / N * 2\n",
    "\n",
    "plt.xlabel('freqency(Hz)', fontsize=14)\n",
    "plt.ylabel('amplitude', fontsize=14)\n",
    "plt.plot(fq, F_abs_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2_ifft = np.fft.ifft(F)\n",
    "F2_ifft_real = F2_ifft.real * 2\n",
    "\n",
    "IPython.display.Audio(data=F2_ifft_real, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"multi_label\"] = train.apply(lambda x: [x[\"primary_label\"]] + eval(x[\"secondary_labels\"]) ,axis=1)\n",
    "\n",
    "primary_label2ebird_code = {\n",
    "    df[\"primary_label\"].unique()[0]: ebird_code \n",
    "    for ebird_code, df in train[[\"ebird_code\", \"primary_label\"]].groupby(\"ebird_code\")\n",
    "}\n",
    "\n",
    "lst = []\n",
    "for multi_label in train[\"multi_label\"]:\n",
    "    _lst = []\n",
    "    for lab in multi_label:\n",
    "        try:\n",
    "            code = primary_label2ebird_code[lab]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        _lst.append(code)\n",
    "    lst.append(_lst)\n",
    "train[\"multi_ebird_code\"] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type2label(t):\n",
    "    t = t.lower()\n",
    "    d = [int(\"call\" in t), int(\"song\" in t)]\n",
    "    return d\n",
    "\n",
    "train[\"type_label\"] = train[\"type\"].map(type2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"multi_ebird_code\", \"type_label\"]].sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
