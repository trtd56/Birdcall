{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "import soundfile as sf\n",
    "import tqdm\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT =  \"../input/resample_sound_2\"\n",
    "OUTPUT = \"../output/train_img_3\"\n",
    "SAMPLE_RATE = 32_000\n",
    "MU = 256\n",
    "NUM_WORKERS = cpu_count()\n",
    "\n",
    "print(NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spec(audio):\n",
    "    spec = librosa.power_to_db(\n",
    "        librosa.feature.melspectrogram(audio, sr=SAMPLE_RATE, fmin=20, fmax=16000, n_mels=128)\n",
    "    )\n",
    "    return spec.astype(np.float32)\n",
    "\n",
    "def audio2vec(path):\n",
    "    x, _ = sf.read(path)\n",
    "    x_spex = audio_to_spec(x)\n",
    "    np.save(f\"{OUTPUT}/{path.parent.name}/{path.name}.npz\", x_spex)\n",
    "    \n",
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "def audio2pict(path):\n",
    "    x, _ = sf.read(path)\n",
    "    x_spex = audio_to_spec(x)\n",
    "    cv2.imwrite(f\"{OUTPUT}/{path.parent.name}/{path.name}.jpg\", mono_to_color(x_spex))\n",
    "    \n",
    "def audio2quantized(path):\n",
    "    data, _ = librosa.load(path=path, sr=SAMPLE_RATE, mono=True)\n",
    "    mu_x = np.sign(data) * np.log(1 + MU * np.abs(data)) / np.log(MU + 1)\n",
    "    bins = np.linspace(-1, 1, MU)\n",
    "    quantized = np.digitize(mu_x, bins) - 1\n",
    "    quantized = quantized.astype(np.uint8)\n",
    "    np.save(f\"{OUTPUT}/{path.parent.name}/{path.name}.npz\", quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta as td\n",
    "\n",
    "\n",
    "def _stft(y, n_fft, hop_length, win_length):\n",
    "    return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "\n",
    "def _istft(y, hop_length, win_length):\n",
    "    return librosa.istft(y, hop_length, win_length)\n",
    "\n",
    "\n",
    "def _amp_to_db(x):\n",
    "    return librosa.core.amplitude_to_db(x, ref=1.0, amin=1e-20, top_db=80.0)\n",
    "\n",
    "\n",
    "def _db_to_amp(x,):\n",
    "    return librosa.core.db_to_amplitude(x, ref=1.0)\n",
    "\n",
    "\n",
    "def plot_spectrogram(signal, title):\n",
    "    fig, ax = plt.subplots(figsize=(20, 4))\n",
    "    cax = ax.matshow(\n",
    "        signal,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        cmap=plt.cm.seismic,\n",
    "        vmin=-1 * np.max(np.abs(signal)),\n",
    "        vmax=np.max(np.abs(signal)),\n",
    "    )\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_statistics_and_filter(\n",
    "    mean_freq_noise, std_freq_noise, noise_thresh, smoothing_filter\n",
    "):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(20, 4))\n",
    "    plt_mean, = ax[0].plot(mean_freq_noise, label=\"Mean power of noise\")\n",
    "    plt_std, = ax[0].plot(std_freq_noise, label=\"Std. power of noise\")\n",
    "    plt_std, = ax[0].plot(noise_thresh, label=\"Noise threshold (by frequency)\")\n",
    "    ax[0].set_title(\"Threshold for mask\")\n",
    "    ax[0].legend()\n",
    "    cax = ax[1].matshow(smoothing_filter, origin=\"lower\")\n",
    "    fig.colorbar(cax)\n",
    "    ax[1].set_title(\"Filter for smoothing Mask\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def removeNoise(\n",
    "    audio_clip,\n",
    "    noise_clip,\n",
    "    n_grad_freq=2,\n",
    "    n_grad_time=4,\n",
    "    n_fft=2048,\n",
    "    win_length=2048,\n",
    "    hop_length=512,\n",
    "    n_std_thresh=1.5,\n",
    "    prop_decrease=1.0,\n",
    "    verbose=False,\n",
    "    visual=False,\n",
    "):\n",
    "    \"\"\"Remove noise from audio based upon a clip containing only noise\n",
    "\n",
    "    Args:\n",
    "        audio_clip (array): The first parameter.\n",
    "        noise_clip (array): The second parameter.\n",
    "        n_grad_freq (int): how many frequency channels to smooth over with the mask.\n",
    "        n_grad_time (int): how many time channels to smooth over with the mask.\n",
    "        n_fft (int): number audio of frames between STFT columns.\n",
    "        win_length (int): Each frame of audio is windowed by `window()`. The window will be of length `win_length` and then padded with zeros to match `n_fft`..\n",
    "        hop_length (int):number audio of frames between STFT columns.\n",
    "        n_std_thresh (int): how many standard deviations louder than the mean dB of the noise (at each frequency level) to be considered signal\n",
    "        prop_decrease (float): To what extent should you decrease noise (1 = all, 0 = none)\n",
    "        visual (bool): Whether to plot the steps of the algorithm\n",
    "\n",
    "    Returns:\n",
    "        array: The recovered signal with noise subtracted\n",
    "\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        start = time.time()\n",
    "    # STFT over noise\n",
    "    noise_stft = _stft(noise_clip, n_fft, hop_length, win_length)\n",
    "    noise_stft_db = _amp_to_db(np.abs(noise_stft))  # convert to dB\n",
    "    # Calculate statistics over noise\n",
    "    mean_freq_noise = np.mean(noise_stft_db, axis=1)\n",
    "    std_freq_noise = np.std(noise_stft_db, axis=1)\n",
    "    noise_thresh = mean_freq_noise + std_freq_noise * n_std_thresh\n",
    "    if verbose:\n",
    "        print(\"STFT on noise:\", td(seconds=time.time() - start))\n",
    "        start = time.time()\n",
    "    # STFT over signal\n",
    "    if verbose:\n",
    "        start = time.time()\n",
    "    sig_stft = _stft(audio_clip, n_fft, hop_length, win_length)\n",
    "    sig_stft_db = _amp_to_db(np.abs(sig_stft))\n",
    "    if verbose:\n",
    "        print(\"STFT on signal:\", td(seconds=time.time() - start))\n",
    "        start = time.time()\n",
    "    # Calculate value to mask dB to\n",
    "    mask_gain_dB = np.min(_amp_to_db(np.abs(sig_stft)))\n",
    "    if verbose:\n",
    "        print(noise_thresh, mask_gain_dB)\n",
    "    # Create a smoothing filter for the mask in time and frequency\n",
    "    smoothing_filter = np.outer(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.linspace(0, 1, n_grad_freq + 1, endpoint=False),\n",
    "                np.linspace(1, 0, n_grad_freq + 2),\n",
    "            ]\n",
    "        )[1:-1],\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.linspace(0, 1, n_grad_time + 1, endpoint=False),\n",
    "                np.linspace(1, 0, n_grad_time + 2),\n",
    "            ]\n",
    "        )[1:-1],\n",
    "    )\n",
    "    smoothing_filter = smoothing_filter / np.sum(smoothing_filter)\n",
    "    # calculate the threshold for each frequency/time bin\n",
    "    db_thresh = np.repeat(\n",
    "        np.reshape(noise_thresh, [1, len(mean_freq_noise)]),\n",
    "        np.shape(sig_stft_db)[1],\n",
    "        axis=0,\n",
    "    ).T\n",
    "    # mask if the signal is above the threshold\n",
    "    sig_mask = sig_stft_db < db_thresh\n",
    "    if verbose:\n",
    "        print(\"Masking:\", td(seconds=time.time() - start))\n",
    "        start = time.time()\n",
    "    # convolve the mask with a smoothing filter\n",
    "    sig_mask = scipy.signal.fftconvolve(sig_mask, smoothing_filter, mode=\"same\")\n",
    "    sig_mask = sig_mask * prop_decrease\n",
    "    if verbose:\n",
    "        print(\"Mask convolution:\", td(seconds=time.time() - start))\n",
    "        start = time.time()\n",
    "    # mask the signal\n",
    "    sig_stft_db_masked = (\n",
    "        sig_stft_db * (1 - sig_mask)\n",
    "        + np.ones(np.shape(mask_gain_dB)) * mask_gain_dB * sig_mask\n",
    "    )  # mask real\n",
    "    sig_imag_masked = np.imag(sig_stft) * (1 - sig_mask)\n",
    "    sig_stft_amp = (_db_to_amp(sig_stft_db_masked) * np.sign(sig_stft)) + (\n",
    "        1j * sig_imag_masked\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Mask application:\", td(seconds=time.time() - start))\n",
    "        start = time.time()\n",
    "    # recover the signal\n",
    "    recovered_signal = _istft(sig_stft_amp, hop_length, win_length)\n",
    "    recovered_spec = _amp_to_db(\n",
    "        np.abs(_stft(recovered_signal, n_fft, hop_length, win_length))\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Signal recovery:\", td(seconds=time.time() - start))\n",
    "    if visual:\n",
    "        plot_spectrogram(noise_stft_db, title=\"Noise\")\n",
    "    if visual:\n",
    "        plot_statistics_and_filter(\n",
    "            mean_freq_noise, std_freq_noise, noise_thresh, smoothing_filter\n",
    "        )\n",
    "    if visual:\n",
    "        plot_spectrogram(sig_stft_db, title=\"Signal\")\n",
    "    if visual:\n",
    "        plot_spectrogram(sig_mask, title=\"Mask applied\")\n",
    "    if visual:\n",
    "        plot_spectrogram(sig_stft_db_masked, title=\"Masked signal\")\n",
    "    if visual:\n",
    "        plot_spectrogram(recovered_spec, title=\"Recovered spectrogram\")\n",
    "    return recovered_signal\n",
    "\n",
    "def envelope(y, rate, threshold):\n",
    "    mask = []\n",
    "    y = pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/20),min_periods=1,center=True).max()\n",
    "    for mean in y_mean:\n",
    "        if mean > threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask, y_mean\n",
    "\n",
    "def audio2pict_denonise(path):\n",
    "    x, _ = librosa.load(path=path, sr=SAMPLE_RATE, mono=True)\n",
    "    mask, env = envelope(x, SAMPLE_RATE, threshold=0.05)\n",
    "    try:\n",
    "        x = removeNoise(audio_clip=x, noise_clip=x[np.logical_not(mask)],verbose=False,visual=False)\n",
    "    except:\n",
    "        pass\n",
    "    x_spex = audio_to_spec(x)\n",
    "    cv2.imwrite(f\"{OUTPUT}/{path.parent.name}/{path.name}.jpg\", mono_to_color(x_spex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "recs = defaultdict(list)\n",
    "for directory in Path(INPUT).iterdir():\n",
    "    if directory.name in [\".DS_Store\"]:\n",
    "        continue\n",
    "    dir_paths = [f for f in directory.iterdir() if f.name not in [\".DS_Store\", \"train_mod.csv\"]]\n",
    "    for dname in tqdm.tqdm_notebook(dir_paths, total=len(dir_paths)):\n",
    "        file_paths = [f for f in dname.iterdir() if f.name != \".DS_Store\"]\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = file_paths[0]\n",
    "fname = path.name.split(\".\")[0]\n",
    "display(train_df.query(f\"filename=='{fname}.mp3'\")[[\"rating\", \"type\", \"primary_label\", \"secondary_labels\"]])\n",
    "x, _ = librosa.load(path=path, sr=SAMPLE_RATE, mono=True)\n",
    "x_spex = audio_to_spec(x)\n",
    "plt.plot(x);plt.show()\n",
    "plt.imshow(mono_to_color(x_spex))\n",
    "IPython.display.Audio(data=x, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, env = envelope(x, SAMPLE_RATE, threshold=0.05)\n",
    "plt.plot(x[mask])\n",
    "plt.plot(x[np.logical_not(mask)]);plt.show()\n",
    "\n",
    "#plt.imshow(mono_to_color(audio_to_spec(x[mask])));plt.show()\n",
    "#plt.imshow(mono_to_color(audio_to_spec(x[np.logical_not(mask)])));plt.show()\n",
    "\n",
    "#IPython.display.Audio(data=x[mask], rate=SAMPLE_RATE)\n",
    "#IPython.display.Audio(data=x[np.logical_not(mask)], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = removeNoise(audio_clip=x, noise_clip=x[np.logical_not(mask)],verbose=True,visual=True)\n",
    "#output = removeNoise(audio_clip=x, noise_clip=x[mask],verbose=True,visual=True)\n",
    "\n",
    "IPython.display.Audio(data=output, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_spex[:, :1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_call = (pd.Series(mask).rolling(SAMPLE_RATE*5).max() == 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(is_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(audio_to_spec(output)[:, :1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask, env = envelope(output, SAMPLE_RATE, threshold=0.05)\n",
    "#plt.plot(output[mask]);plt.show()\n",
    "#plt.imshow(mono_to_color(audio_to_spec(output[mask])));plt.show()\n",
    "#IPython.display.Audio(data=output[mask], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recs = defaultdict(list)\n",
    "for directory in Path(INPUT).iterdir():\n",
    "    if directory.name in [\".DS_Store\"]:\n",
    "        continue\n",
    "    dir_paths = [f for f in directory.iterdir() if f.name not in [\".DS_Store\", \"train_mod.csv\"]]\n",
    "    for dname in tqdm.tqdm_notebook(dir_paths, total=len(dir_paths)):\n",
    "        file_paths = [f for f in dname.iterdir() if f.name != \".DS_Store\"]\n",
    "        !mkdir -p \"{OUTPUT}/{dname.name}\"\n",
    "        with Pool(NUM_WORKERS // 2) as p:\n",
    "            #p.map(audio2pict, file_paths)\n",
    "            p.map(audio2pict_denonise, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = defaultdict(list)\n",
    "for directory in tqdm.tqdm_notebook(Path(INPUT).iterdir(), total=len(os.listdir(INPUT))):\n",
    "    if directory.name == \".DS_Store\":\n",
    "        continue\n",
    "    !mkdir -p \"{OUTPUT}/{directory.name}\"\n",
    "    file_paths = [f for f in directory.iterdir() if f.name != \".DS_Store\"]\n",
    "    with Pool(NUM_WORKERS // 2) as p:\n",
    "        p.map(audio2vec, file_paths)\n",
    "        #p.map(audio2pict, file_paths)\n",
    "        #p.map(audio2quantized, file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check ignore files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in tqdm.tqdm_notebook(Path(OUTPUT).iterdir(), total=len(os.listdir(OUTPUT))):\n",
    "    if directory.name == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = [f for f in directory.iterdir() if f.name != \".DS_Store\"]\n",
    "    for path in file_paths:\n",
    "        size = os.path.getsize(path)\n",
    "        if size < 1:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    f\"{INPUT}/comrav/XC246425.wav\",\n",
    "    f\"{INPUT}/prawar/XC479026.wav\",\n",
    "    f\"{INPUT}/snobun/XC487557.wav\",\n",
    "    f\"{INPUT}/snobun/XC487556.wav\",\n",
    "    f\"{INPUT}/stejay/XC503349.wav\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = sf.read(paths[0])\n",
    "x_spex = audio_to_spec(x)\n",
    "\n",
    "print(x_spex.shape)\n",
    "cv2.imwrite(f\"tmp.jpg\", mono_to_color(x_spex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"../output/train_npz/aldfly/XC134874.wav.npz.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = librosa.feature.melspectrogram(arr.astype(float), sr=SAMPLE_RATE, fmin=20, fmax=16000, n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(spect.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(spect)\n",
    "x = x.unsqueeze(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.nn.MaxPool1d(32)(x)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h[0].numpy().max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h[0].numpy().max(0) > 0.1*1e7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = defaultdict(list)\n",
    "for directory in tqdm.tqdm_notebook(Path(INPUT).iterdir(), total=len(os.listdir(INPUT))):\n",
    "    if directory.name == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = [f for f in directory.iterdir() if f.name != \".DS_Store\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = file_paths[4]\n",
    "path = \"../input/train_resampled/ameavo/XC304534.wav\"\n",
    "print(path)\n",
    "data, sr = librosa.load(path=path, sr=SAMPLE_RATE, mono=True)\n",
    "plt.plot(data[:160000], ',', linestyle=\"None\");plt.show()\n",
    "\n",
    "x_spex = audio_to_spec(data)\n",
    "pct = mono_to_color(x_spex)\n",
    "plt.imshow(pct);plt.show()\n",
    "\n",
    "IPython.display.Audio(data=data, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "dt = 32000\n",
    "\n",
    "F = np.fft.fft(data)\n",
    "F_abs = np.abs(F)\n",
    "F_abs_amp = F_abs / N * 2\n",
    "\n",
    "fq = np.linspace(0, 1.0/dt, N)\n",
    "\n",
    "plt.xlabel('freqency(Hz)', fontsize=14)\n",
    "plt.ylabel('amplitude', fontsize=14)\n",
    "plt.plot(fq, F_abs_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 1e-5 # カットオフ（周波数）\n",
    "F[(fq > fc)] = 0\n",
    "\n",
    "#ac = 0.00002 # 振幅強度の閾値\n",
    "#F[(F_abs_amp < ac)] = 0\n",
    "\n",
    "F_abs = np.abs(F)\n",
    "F_abs_amp = F_abs / N * 2\n",
    "\n",
    "plt.xlabel('freqency(Hz)', fontsize=14)\n",
    "plt.ylabel('amplitude', fontsize=14)\n",
    "plt.plot(fq, F_abs_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2_ifft = np.fft.ifft(F)\n",
    "F2_ifft_real = F2_ifft.real * 2\n",
    "\n",
    "IPython.display.Audio(data=F2_ifft_real, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"multi_label\"] = train.apply(lambda x: [x[\"primary_label\"]] + eval(x[\"secondary_labels\"]) ,axis=1)\n",
    "\n",
    "primary_label2ebird_code = {\n",
    "    df[\"primary_label\"].unique()[0]: ebird_code \n",
    "    for ebird_code, df in train[[\"ebird_code\", \"primary_label\"]].groupby(\"ebird_code\")\n",
    "}\n",
    "\n",
    "lst = []\n",
    "for multi_label in train[\"multi_label\"]:\n",
    "    _lst = []\n",
    "    for lab in multi_label:\n",
    "        try:\n",
    "            code = primary_label2ebird_code[lab]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        _lst.append(code)\n",
    "    lst.append(_lst)\n",
    "train[\"multi_ebird_code\"] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type2label(t):\n",
    "    t = t.lower()\n",
    "    d = [int(\"call\" in t), int(\"song\" in t)]\n",
    "    return d\n",
    "\n",
    "train[\"type_label\"] = train[\"type\"].map(type2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"multi_ebird_code\", \"type_label\"]].sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
